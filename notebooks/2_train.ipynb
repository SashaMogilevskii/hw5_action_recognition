{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт либ\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from box import Box\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.io import read_video\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name_video</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>videos/video_0000.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>videos/video_0001.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>videos/video_0002.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>videos/video_0003.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>videos/video_0004.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             name_video        label\n",
       "0           0  videos/video_0000.mp4  tap dancing\n",
       "1           1  videos/video_0001.mp4  tap dancing\n",
       "2           2  videos/video_0002.mp4  tap dancing\n",
       "3           3  videos/video_0003.mp4  tap dancing\n",
       "4           4  videos/video_0004.mp4  tap dancing"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на обновленную нашу дату \n",
    "df = pd.read_csv(\"../data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name_video</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>videos/video_0000.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>videos/video_0001.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>videos/video_0002.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>videos/video_0003.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>videos/video_0004.mp4</td>\n",
       "      <td>tap dancing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             name_video        label  target\n",
       "0           0  videos/video_0000.mp4  tap dancing       0\n",
       "1           1  videos/video_0001.mp4  tap dancing       0\n",
       "2           2  videos/video_0002.mp4  tap dancing       0\n",
       "3           3  videos/video_0003.mp4  tap dancing       0\n",
       "4           4  videos/video_0004.mp4  tap dancing       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Закодируем лейблы в числовые значения \n",
    "unique_labels = df['label'].unique()\n",
    "label_dict = {label: index for index, label in enumerate(unique_labels)}\n",
    "df['target'] = df.label.map(label_dict)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cоздадим конфиг для обучения модели\n",
    "config = Box()\n",
    "\n",
    "config.num_workers = 1\n",
    "config.batch_size = 24\n",
    "config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config.seed = 1771\n",
    "config.model_name = 'mc3_18'\n",
    "config.num_features = df.target.nunique()\n",
    "config.optimizer_lr = 0.0001\n",
    "config.epochs = 15\n",
    "config.test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vidaug import augmentors as va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напишем датаcэт для наших данных \n",
    "\n",
    "class DanceDanceDataset(Dataset):\n",
    "    def __init__(self, df, is_train = False):\n",
    "        self.df = df\n",
    "        self.video_path = \"..\"\n",
    "        self.is_train = is_train\n",
    "        \n",
    "#         sometimes = lambda aug: va.Sometimes(0.5, aug)\n",
    "#         self.aug = va.Sequential([ \n",
    "#             va.RandomRotate(degrees=(-40, 40)),  \n",
    "#             sometimes(va.HorizontalFlip())])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        target = row['target']\n",
    "        \n",
    "        video_path = os.path.join(self.video_path, row['name_video'])\n",
    "\n",
    "        video, audio, info = read_video(video_path, pts_unit=\"sec\")\n",
    "        # Беру только 16 кадров. \n",
    "        if len(video) > 0:\n",
    "            if len(video) < 48:\n",
    "                video = video[:16] \n",
    "            else:\n",
    "                video = video[:48:3]\n",
    "            if self.is_train:\n",
    "                video = video.numpy()\n",
    "                video = self.aug(video)\n",
    "                video = torch.Tensor(video)\n",
    "            resize_transform = transforms.Resize((112, 112))\n",
    "            video_resized = torch.stack([resize_transform(frame.permute(2, 0, 1)).permute(1, 2, 0) for frame in video])\n",
    "            video_normalized = video_resized.permute(3, 0, 1, 2) \n",
    "\n",
    "            # Лениво нормализую \n",
    "            tensor_3d = video_normalized / 255 \n",
    "        else:\n",
    "            tensor_3d = torch.empty(3, 16, 112, 112)\n",
    "            \n",
    "        label = torch.tensor(target).long()\n",
    "        return tensor_3d, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 112, 112])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = DanceDanceDataset(df.reset_index())\n",
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разобьем наши данные на тест и трейн. cоздадим тренеровочный и тестовый датасэт и даталоадэры\n",
    "train_df, val_df = train_test_split(df, \n",
    "                                    test_size=config.test_size,\n",
    "                                    random_state=config.seed,\n",
    "                                    stratify=df['target']\n",
    "                                   )\n",
    "dataset_train = DanceDanceDataset(train_df.reset_index())\n",
    "dataset_test = DanceDanceDataset(val_df.reset_index())\n",
    "\n",
    "train_loader = DataLoader(dataset_train,\n",
    "                          batch_size=config.batch_size,\n",
    "                          shuffle=True,\n",
    "#                          num_workers=config.num_workers\n",
    "                         )\n",
    "valid_loader = DataLoader(dataset_test,\n",
    "                          batch_size=config.batch_size,\n",
    "#                          num_workers=config.num_workers\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.video.mc3_18.html#mc3-18\n",
    "\n",
    "# Загрузка предобученной модели mc3_18\n",
    "model = models.video.mc3_18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, config.num_features)\n",
    "model.to(config.device)\n",
    "config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.optimizer_lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------epoch:1/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [16:27<00:00, 12.19s/it, gpu_load=5.11GB, loss=1.9107]\n",
      "Testing: 100%|██████████| 21/21 [02:27<00:00,  7.01s/it, loss=1.5207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, lr_rate 0.0001\n",
      "loss_train: 2.0723| loss_valid: 1.6556|\n",
      "metric 0.514403\n",
      "Elapsed time: 00:18:54\n",
      "---------------------epoch:2/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [12:16<00:00,  9.10s/it, gpu_load=5.11GB, loss=0.9362]\n",
      "Testing: 100%|██████████| 21/21 [00:55<00:00,  2.64s/it, loss=1.2006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, lr_rate 0.0001\n",
      "loss_train: 1.1971| loss_valid: 1.4653|\n",
      "metric 0.54321\n",
      "Elapsed time: 00:13:12\n",
      "---------------------epoch:3/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [16:31<00:00, 12.24s/it, gpu_load=5.11GB, loss=0.7329]\n",
      "Testing: 100%|██████████| 21/21 [01:04<00:00,  3.08s/it, loss=1.1129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, lr_rate 0.0001\n",
      "loss_train: 0.7036| loss_valid: 1.4358|\n",
      "metric 0.55144\n",
      "Elapsed time: 00:17:35\n",
      "---------------------epoch:4/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [03:38<00:00,  2.70s/it, gpu_load=5.11GB, loss=0.4108]\n",
      "Testing: 100%|██████████| 21/21 [00:52<00:00,  2.49s/it, loss=0.9564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, lr_rate 8e-05\n",
      "loss_train: 0.3577| loss_valid: 1.4219|\n",
      "metric 0.541152\n",
      "Elapsed time: 00:04:30\n",
      "---------------------epoch:5/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [04:02<00:00,  2.99s/it, gpu_load=5.11GB, loss=0.1264]\n",
      "Testing: 100%|██████████| 21/21 [00:51<00:00,  2.47s/it, loss=0.9055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, lr_rate 8e-05\n",
      "loss_train: 0.1898| loss_valid: 1.4491|\n",
      "metric 0.534979\n",
      "Elapsed time: 00:04:54\n",
      "---------------------epoch:6/15---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 81/81 [03:35<00:00,  2.66s/it, gpu_load=5.11GB, loss=0.0775]\n",
      "Testing:  33%|███▎      | 7/21 [00:32<01:04,  4.63s/it, loss=1.4996]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m valid_pbar \u001b[38;5;241m=\u001b[39m tqdm(valid_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m (valid_pbar):\n\u001b[0;32m     42\u001b[0m         X_batch \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     43\u001b[0m         y_batch \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36mDanceDanceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m     video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(video)\n\u001b[0;32m     34\u001b[0m resize_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m))\n\u001b[1;32m---> 35\u001b[0m video_resized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([resize_transform(frame\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video])\n\u001b[0;32m     36\u001b[0m video_normalized \u001b[38;5;241m=\u001b[39m video_resized\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Лениво нормализую \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 35\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m     video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(video)\n\u001b[0;32m     34\u001b[0m resize_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m))\n\u001b[1;32m---> 35\u001b[0m video_resized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mresize_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video])\n\u001b[0;32m     36\u001b[0m video_normalized \u001b[38;5;241m=\u001b[39m video_resized\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Лениво нормализую \u001b[39;00m\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torchvision\\transforms\\functional.py:492\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:472\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m--> 472\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43m_cast_squeeze_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_cast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_cast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_squeeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_squeeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mE:\\Project_python\\hw_action_recognition\\myenv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:539\u001b[0m, in \u001b[0;36m_cast_squeeze_out\u001b[1;34m(img, need_cast, need_squeeze, out_dtype)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_cast:\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39muint8, torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39mint16, torch\u001b[38;5;241m.\u001b[39mint32, torch\u001b[38;5;241m.\u001b[39mint64):\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;66;03m# it is better to round before cast\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(out_dtype)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Проведем обучение модели. Для корректной работы и для защиты от сбоев будем сохранять модель после каждой эпохи\n",
    "for epoch_i in range(1, config.epochs + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    print(f'---------------------epoch:{epoch_i}/{config.epochs}---------------------')\n",
    "\n",
    "    # loss\n",
    "    avg_train_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    summa = 0\n",
    "    ############## Train #############\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for X,y in (train_pbar):\n",
    "        X_batch = X.to(config.device)\n",
    "        y_batch = y.to(config.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        res = model.forward(X_batch)\n",
    "    \n",
    "        loss = loss_f(res, y_batch)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            train_pbar.set_postfix(gpu_load=f\"{torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB\",\n",
    "                                   loss=f\"{loss.item():.4f}\")\n",
    "        else:\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_train_loss += loss * len(y_batch)\n",
    "\n",
    "        del X, res\n",
    "        \n",
    "\n",
    "    \n",
    "    ########## VALIDATION ###############\n",
    "    model.eval()\n",
    "    valid_pbar = tqdm(valid_loader, desc=\"Testing\")\n",
    "    with torch.no_grad():\n",
    "        for X,y in (valid_pbar):\n",
    "            X_batch = X.to(config.device)\n",
    "            y_batch = y.to(config.device)\n",
    "\n",
    "            res = model.forward(X_batch)\n",
    "            \n",
    "            loss = loss_f(res, y_batch)\n",
    "            avg_val_loss += loss * len(y_batch)\n",
    "            valid_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "            res = res.detach().cpu()\n",
    "            y_batch = y_batch.cpu()\n",
    "            \n",
    "            preds = torch.max(F.softmax(res, dim=1), dim=1)\n",
    "            correct= torch.eq(preds[1], y_batch)\n",
    "            summa += torch.sum(correct).item()\n",
    "\n",
    "            del X, res\n",
    "            \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    avg_train_loss = avg_train_loss / len(dataset_train)\n",
    "    avg_val_loss = avg_val_loss / len(dataset_test)\n",
    "    \n",
    "    acc = summa / len(dataset_test)\n",
    "\n",
    "    print(f'epoch: {epoch_i}, lr_rate {optimizer.param_groups[0][\"lr\"]}')\n",
    "\n",
    "    print(\"loss_train: %0.4f| loss_valid: %0.4f|\" % (avg_train_loss, avg_val_loss))\n",
    "    print(f\"metric {acc:.<5g}\")\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    hours = int(elapsed_time // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    print(f\"Elapsed time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "    scheduler.step()\n",
    "    torch.save(model, f\"model_ep_{epoch_i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики стали падать - сеть переобучилась. Преостанавливаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
